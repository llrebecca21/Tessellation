---
title: "Tessellation README"
output:
  pdf_document: default
  html_document: default
date: "2023-01-23"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Metropolis Hastings and Gibbs Derivation


First, following the steps in AdaptSPEC, we rewrite the Whittle Likelihood as follows using the notation change that $y_n(\omega_j) = \log(I_n(\omega_j))$:

\begin{align*}
p(x|f) &= (2\pi)^{n/2} \prod_{j=0}^{n-1}\exp\{-\frac{1}{2} [\log f(\omega_j) + I_n(\omega_j)/f(\omega_j)]\}\\
&= (2\pi)^{-n/2}\prod_{j=0}^{n-1}\exp\left\{-\frac{1}{2}\left[\log (f(\omega_j)) + \frac{\exp\{\log(I_n(\omega_j))\}}{\exp\{\log(f(\omega_j))\}}\right]\right\} \\
\end{align*}

Then we use the equation given in the paper: $\log (f_j) = g(\omega_j) = \alpha_0 + h(\omega_j) = \alpha_0 + X_j^T \beta$ in order to replace the unknown spectral density with $\alpha_0$'s and $\beta$'s

\begin{align*}
&= (2\pi)^{-n/2}\prod_{j=0}^{n-1}\exp\left\{-\frac{1}{2}\left[\log (f(\omega_j)) + \frac{\exp\{y_n(\omega_j)\}}{\exp\{\log(f(\omega_j))\}}\right]\right\} \\
\mathcal{L}(\alpha_0,\beta,\tau^2 | y)&= (2\pi)^{-n/2}\prod_{j=0}^{n-1}\exp\left\{-\frac{1}{2}\left[ \alpha_0 + X_j^T \beta + \frac{\exp\{y_j\}}{\exp\{\alpha_0 + X_j^T \beta\}} \right]\right\}\\
&= (2\pi)^{-n/2}\prod_{j=0}^{n-1} \exp\left\{-\frac{1}{2} \left[ \alpha_0 + X_j^T\beta + \exp\{y_j - \alpha_0 - X_j^T \beta\} \right]\right\} \\
&= (2\pi)^{-n/2}\exp\left\{-\frac{1}{2}[\alpha_0 n + 1_n X\beta + \sum^{n-1}_{j=0}\exp\{y_j-\alpha_0 - X^T_j \beta\}]\right\} \\
\end{align*}

### Priors 

Following closely* the priors outlined in AdaptSPEC, we give the parameters the following priors:
\begin{align*}
\pi(\alpha) &\sim N(0,\sigma^2_\alpha) \\
\pi(\beta) &\sim N(0,\tau^2,D_K) \\
\pi(\tau^2 | \lambda) &\sim IG\left(\frac{\nu_0}{2}, \frac{\nu_0}{\lambda}\right) \\
\pi(\lambda) &\sim IG\left(\frac{1}{2},\frac{1}{\eta_0^2}\right) \\
\end{align*}

Such that $D_K$ is a positive definite diagonal matrix of dimension $K$. 

 *Instead of giving $\tau^2$ a Uniform prior as in AdaptSPEC we have given it a half-t prior. Following the work laid out in Wand et.al. we are able to write the half-t prior as an Inverse-Gamma PDF with the parameters as written above.

### Posterior Distribution

Now, we go through the derivation of calculating the full posterior distribution of the Whittle Likelihood:
\begin{align*}
\pi(\beta, \tau^2, \alpha_0, \lambda | y) \propto \mathcal{L}(\beta, \tau^2, \alpha_0, \lambda | y) \cdot \pi(\beta) \cdot \pi(\tau^2 | \lambda) \cdot \pi(\lambda) \cdot \pi(\alpha_0)
\end{align*}

In order to be precise and organized we will first take each prior given and rewrite the prior as it will be used in the posterior distribution.

First, $\pi(\alpha_0) \sim N(0, \sigma^2_\alpha)$:
\begin{align*}
\pi(\alpha_0) &= \frac{1}{\sqrt{2\pi \sigma^2}}\exp\left\{-\frac{\alpha_0^2}{2\sigma_\alpha^2}\right\} \\
&\propto \exp\left\{-\frac{\alpha_0^2}{2\sigma^2_{\alpha}}\right\}
\end{align*}

Second, $\pi(\beta) \sim N(0, \tau^2D_k)$:
\begin{align*}
\pi(\beta) &= \frac{1}{\sqrt{(2\pi)^K |\tau^2 D_K|}}\exp\left\{-\frac{1}{2}\beta^T(\tau^2D_K)^{-1}\beta\right\}\\
&\propto (\tau^2)^{-K/2} \cdot \exp\left\{-\frac{1}{2}\beta^T(\tau^2D_K)^{-1}\beta\right\}
\end{align*}

Third, $\pi(\tau^2 | \lambda) \sim IG\left(\frac{\nu_0}{2}, \frac{\nu_0}{\lambda}\right)$:
\begin{align*}
\pi(\tau^2 | \lambda) &= \frac{\left(\frac{\nu_0}{\lambda}\right)^{\nu_0/2}}{\Gamma\left(\frac{\nu_0}{2}\right)}\cdot\left(\frac{1}{\tau^2}\right)^{\frac{\nu_0}{2} + 1}\cdot \exp\left\{-\frac{\nu_0}{\tau^2\lambda}\right\}\\
&\propto \frac{1}{\lambda^{\nu_0/2}}\left(\frac{1}{\tau^2}\right)^{\frac{\nu_0}{2} + 1}\cdot \exp\left\{-\frac{\nu_0}{\tau^2\lambda}\right\}
\end{align*}

Fourth, $\pi(\lambda) \sim IG\left(\frac{1}{2},\frac{1}{\eta_0^2}\right)$:
\begin{align*}
\pi(\lambda) &= \frac{\left(\frac{1}{\eta_0^2}\right)^{1/2}}{\Gamma(1/2)}\left(\frac{1}{\lambda}\right)^{3/2}\exp\left\{-\frac{1}{\lambda\eta_0^2}\right\} \\
&\propto \left(\frac{1}{\lambda}\right)^{3/2} \exp\left\{-\frac{1}{\lambda\eta_0^2}\right\}
\end{align*}

Now, putting this together we have:
\begin{align*}
\pi(\beta, \tau^2, \alpha_0, \lambda | y)  &\propto (2\pi)^{-n/2}\exp\left\{-\frac{1}{2}[\alpha_0 n + 1_n X\beta + \sum^{n-1}_{j=0}\exp\{y_j-\alpha_0 - X^T_j \beta\}]\right\} \cdot \exp\left\{-\frac{\alpha_0^2}{2\sigma^2_{\alpha}}\right\} \\
&\cdot (\tau^2)^{-K/2} \cdot \exp\left\{-\frac{1}{2}\beta^T(\tau^2D_K)^{-1}\beta\right\} \cdot \frac{1}{\lambda^{\nu_0/2}}\left(\frac{1}{\tau^2}\right)^{\frac{\nu_0}{2} + 1}\cdot \exp\left\{-\frac{\nu_0}{\tau^2\lambda}\right\} \cdot \left(\frac{1}{\lambda}\right)^{3/2} \exp\left\{-\frac{1}{\lambda\eta_0^2}\right\}
\end{align*}

## Conditional Posterior Distributions

### Conditional Posterior Distribution of $\beta$ and $\alpha_0$

\begin{align*}
\pi(\beta,\alpha_0 | y) \propto \exp\left\{-\frac{1}{2}\left[\alpha_0 n + \frac{\alpha_0^2}{\sigma^2_{\alpha}} + 1_{n}X\beta + \frac{1}{\tau^2}\beta^T(D_K)^{-1}\beta + \sum^{n-1}_{j=0} \exp\left\{y_j - \alpha_0 - X^T_j\beta\right\}\right]\right\}
\end{align*}

### Conditional Posterior Distribution of $\tau^2$

\begin{align*}
\pi(\tau^2 | y) &\propto (\tau^2)^{-K/2} \cdot \exp\left\{-\frac{1}{2}\beta^T(\tau^2 D_K)^{-1}\beta\right\} \cdot \left(\frac{1}{\tau^2}\right)^{\frac{\nu_0}{2}+1} \cdot \exp\left\{-\frac{\nu_0}{\tau^2 \lambda}\right\}\\
&\propto (\tau^2)^{-\frac{K - \nu_0}{2} - 1} \cdot \exp\left\{-\frac{1}{\tau^2}\left[\frac{1}{2}\beta^T(D_K)^{-1}\beta + \frac{\nu_0}{\lambda}\right]\right\}
\end{align*}

Hence, $\pi(\tau^2 | y) \sim IG\left( \frac{K + \nu_0}{2} \;,\; \frac{1}{2}\beta^T(D_K)^{-1}\beta + \frac{\nu_0}{\lambda}\right)$

### Conditional Posterior Distribution of $\lambda$

\begin{align*}
\pi(\lambda | y) &\propto \frac{1}{\lambda^{\nu_0/2}} \cdot \exp\left\{-\frac{\nu_0}{\tau^2\lambda}\right\} \cdot \left(\frac{1}{\lambda}\right)^{3/2} \cdot \exp\left\{-\frac{1}{\lambda \eta_0^2}\right\} \\
&\propto \frac{1}{\lambda^{\nu_0/2}\lambda^{3/2}}\cdot \exp\left\{-\frac{\nu_0}{\tau^2\lambda} - \frac{1}{\lambda \eta^2_0}\right\} \\
&\propto \lambda^{-\left(\frac{\nu_0 + 1}{2}\right) - 1}\cdot \exp\left\{-\frac{1}{\lambda}\left[\frac{\nu_0}{\tau^2} + \frac{1}{\eta_0^2}\right]\right\}
\end{align*}

Hence, $\pi(\lambda | y) \sim IG\left(\frac{\nu_0 + 1}{2} \;,\; \frac{\nu_0}{\tau^2} + \frac{1}{\eta_0^2}\right)$

## Gradient and Hessian of $\pi(\alpha_0, \beta | y)$

### Gradient of $\pi(\alpha_0, \beta | y)$

\begin{equation*}
\bigtriangledown \log \pi(\alpha_0, \beta | y) = \bigtriangledown \log f(\cdot) =
\begin{bmatrix} \frac{\partial}{\partial \alpha_0} \log f(\alpha_0, \beta)\\
\bigtriangledown_\beta \log f(\alpha_0, \beta) \\
\end{bmatrix}
\end{equation*}

\begin{align*}
\frac{\partial}{\partial \alpha_0} \log f(\alpha_0, \beta) &= \frac{\partial}{\partial \alpha_0} \left[-\frac{1}{2} \left[ \alpha_0 n + \frac{\alpha_0^2}{\sigma^2_{\alpha_0}} + 1_n X\beta + \frac{1}{\tau^2} \beta^T (D_K)^{-1} \beta + \sum^{n-1}_{j=0} \exp\{y_j - \alpha_0 - X^T_j \beta\}\right] \right] \\
&= -\frac{1}{2} \left[ n + \frac{2 \alpha_0}{\sigma^2_{\alpha_0}} + \sum_{j=0}^{n-1} - \exp\{y_j - \alpha_0 - X^T_j \beta\} \right] \\
&= - \frac{n}{2} - \frac{\alpha_0}{\sigma^2_{\alpha}} + \frac{1}{2} \sum_{j=0}^{n-1} \exp\{y_j - \alpha_0 - X_j^T\beta\} \\
\end{align*}

\begin{align*}
\bigtriangledown_\beta \log f(\alpha_0, \beta) &= \frac{\partial}{\partial \beta} \left[ - \frac{1}{2} \left[ 1_n X \beta + \frac{1}{\tau^2} \beta^T (D_K)^{-1} \beta + \sum_{j=0}^{n-1} \exp\{y_j - \alpha_0 - X^T_j \beta\}\right]\right] \\
&= \frac{\partial}{\partial \beta} \left[-\frac{1}{2} 1_n X \beta \right] + \frac{\partial}{\partial \beta} \left[ -\frac{1}{2\tau^2} \beta^T (D_K)^{-1} \beta \right] + \frac{\partial}{\partial \beta} \left[ -\frac{1}{2} \sum^{n-1}_{j=0} \exp\{y_j - \alpha_0 - X^T_j \beta\}\right]\\
&= -\frac{1}{2} \left[ X^T 1_n^T + 2D^{-1}_K \beta + \sum^{n-1}_{j=0} -X_j \exp\{y_j - \alpha_0 - X_j^T\beta\}\right]
\end{align*}

Therefore, the gradient of the conditional posterior of $\beta$ and $\alpha_0$ conditioned on $y$ is :

\begin{equation*}
\bigtriangledown \log \pi(\alpha_0, \beta | y) = \bigtriangledown \log f(\cdot) =
\begin{bmatrix} - \frac{n}{2} - \frac{\alpha_0}{\sigma^2_{\alpha}} + \frac{1}{2} \sum_{j=0}^{n-1} \exp\{y_j - \alpha_0 - X_j^T\beta\} \\
-\frac{1}{2} \left[ X^T 1_n^T + 2D^{-1}_K \beta + \sum^{n-1}_{j=0} -X_j \exp\{y_j - \alpha_0 - X_j^T\beta\}\right] \\
\end{bmatrix}
\end{equation*}

### Hessian of $\pi(\beta, \alpha_0 | y)$

\begin{equation*}
\bigtriangledown^2 \log f(\cdot) =
\begin{bmatrix}
\frac{\partial^2}{\partial \alpha_0^2} & \left(\frac{\partial^2}{\partial \alpha_0 \partial \beta}\right)^T \\
\frac{\partial^2}{\partial \beta \partial \alpha_0} & \bigtriangledown^2_{\beta} f\\
\end{bmatrix}
\end{equation*}

\begin{align*}
\frac{\partial^2\log f}{\partial \alpha_0^2} &= \frac{\partial}{\partial \alpha_0} \left[ -\frac{n}{2} - \frac{\alpha_0}{\sigma^2_{\alpha}} + \frac{1}{2} \sum^{n-1}_{j=0} \exp \left\{ y_j - \alpha_0 - X_j^T \beta \right\}\right]\\
&= - \frac{1}{\sigma^2_{\alpha}} + \frac{1}{2} \sum^{n-1}_{j=0} - \exp\{y_j - \alpha_0 - X^T_j \beta\}\\
&= -\frac{1}{\sigma^2_{\alpha}} - \frac{1}{2}\sum^{n-1}_{j=0} \exp\{y_j - \alpha_0 - X^T_j\beta\}
\end{align*}

\begin{align*}
\frac{\partial^2 \log f}{\partial \beta \partial \alpha_0} &= \frac{\partial}{\partial \alpha} \left[ -\frac{1}{2} X^T 1_n^T - \frac{D^{-1}_K}{\tau^2}\beta + \frac{1}{2} \sum^{n-1}_{j=0} \exp\{y_j - \alpha_0 - X^T_j \beta\} \right] \\
&= -\frac{1}{2} \sum^{n-1}_{j=0} X_j \exp\{y_j - \alpha_0 - X^T_j \beta\}
\end{align*}

\begin{align*}
\frac{\partial^2 \log f}{\partial \alpha_0 \partial \beta} &= \frac{\partial}{\partial \beta} \left[ - \frac{n}{2} - \frac{\alpha_0}{\sigma^2_{\alpha_0}} + \frac{1}{2} \sum^{n-1}_{j=0} \exp\{y_j - \alpha_0 - X^T_j \beta\}\right] \\
&= -\frac{1}{2} \sum^{n-1}_{j=0} X_j \exp\{y_j - \alpha_0 - X^T_j \beta\}
\end{align*}

\begin{align*}
\frac{\partial^2 \log f}{\partial^2 \beta} &= \frac{\partial}{\partial \beta} \left[ -\frac{1}{2} \left[ X^T 1_n^T + \frac{2}{\tau^2}D^{-1}_K \beta + \sum^{n-1}_{j=0} - X_j \exp\{y_j - \alpha_0 - X_j^T \beta \} \right] \right] \\
&= -\frac{D^{-1}_K}{\tau^2} + -\frac{1}{2} \sum^{n-1}_{j=0} X_j X^T_j \exp\{y_j - \alpha_0 - X^T_j \beta\}
\end{align*}
 
Therefore, the Hessian is:

\begin{equation*}
\bigtriangledown^2 \log f(\cdot) =
\begin{bmatrix}
-\frac{1}{\sigma^2_{\alpha}} - \frac{1}{2}\sum^{n-1}_{j=0} \exp\{y_j - \alpha_0 - X^T_j\beta\} & 
\left( -\frac{1}{2} \sum^{n-1}_{j=0} X_j \exp\{y_j - \alpha_0 - X^T_j \beta\} \right)^T \\
-\frac{1}{2} \sum^{n-1}_{j=0} X_j \exp\{y_j - \alpha_0 - X^T_j \beta\} &
-\frac{D^{-1}_K}{\tau^2} + -\frac{1}{2} \sum^{n-1}_{j=0} X_j X^T_j \exp\{y_j - \alpha_0 - X^T_j \beta\} \\
\end{bmatrix}
\end{equation*}
 



